---
title: "p8106_midterm"
author: "Anand Rajan"
date: "3/25/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
```

```{r}
library(tidyverse)
library(caret)
library(janitor)
library(MASS)
library(pROC)
library(dplyr)
library(vip)

```
# Introduction
With the data set, we are trying to answer questions on what factors influence the likelihood of stroke in patients. The data set contains variables on specific health information such as bmi, average glucose level, and heart disease status. Along with health information, the dataset contains information on social factors as well such as marriage status, residence type, and work type. From these variables we are not only going to look at what factors predict stroke, but whether the likelihood of strokes are different across various demographic/social factors. 

```{r}
stroke_data <- read_csv("data/healthcare-dataset-stroke-data.csv") %>%
  janitor::clean_names() %>%
  mutate(
    gender = as.factor(gender),
    hypertension = as.factor(hypertension),
    heart_disease = as.factor(heart_disease),
    ever_married = as.factor(ifelse(ever_married == "Yes", 1, 0)),
    work_type = as.factor(work_type),
    residence_type = as.factor(residence_type),
    bmi = as.double(bmi),
    smoking_status = as.factor(smoking_status),
    stroke = as.factor(stroke)
  ) %>%
  dplyr::select(-id)


stroke_data <- drop_na(stroke_data)
stroke_data


```


To clean the data, we converted all the categorical variables into factors, and converted BMI, a continuous variable, into a dbl. Then we removed the ID variable and any missing observations. 


# Exploratory Analysis 

## Exploring the different demographic variables 


Let's compare stroke prevalence by gender

```{r}
stroke_data %>% 
  group_by(gender) %>% 
  count(stroke) %>% 
  pivot_wider(
    names_from = gender,
    values_from = n 
  ) %>% 
  adorn_totals("row") %>% 
  knitr::kable()
```
 From the table, we see there isn't a major difference between stroke prevalence across gender. The calculated OR is 1.07. Furthermore we see there are significantly more females than males in the population.
 
 Let's look at work type
 
 
```{r}
stroke_data %>% 
  group_by(work_type) %>% 
  count(stroke) %>% 
  pivot_wider(
    names_from = work_type,
    values_from = n 
  ) %>% 
  adorn_totals("row") %>% 
  knitr::kable()

```
 From evaluating the table, we do not see any major differences in stroke prevalence across work types. However, we do see a majority of patients in the data set are privately employed.
 
```{r}
stroke_data %>% 
  group_by(residence_type) %>% 
  count(stroke) %>% 
  pivot_wider(
    names_from = residence_type,
    values_from = n 
  ) %>% 
  adorn_totals("row") %>% 
  knitr::kable()

chisq.test(stroke_data$residence_type, stroke_data$stroke)
```
 
 
 
Now let's look at more health-related factors
 
```{r}
stroke_data %>% 
  group_by(smoking_status) %>%
  count(stroke) %>% 
  pivot_wider(
    names_from = smoking_status,
    values_from = n 
  ) %>% 
  adorn_totals("row") %>%
  knitr::kable()
```
From looking at the table, we do not see major differences in prevalence rate of stroke across smoking status categories. However I want to run additional statistical testing(Chi-squared test) to see if there is a difference across smoking status groups


```{r}
stroke_smoking_df <- stroke_data %>% 
            filter(smoking_status != "Unknown")


chisq.test(stroke_smoking_df$stroke,stroke_smoking_df$smoking_status)
```
We removed observations where their smoking status was unknown and ran a chi-squared test comparing frequency of stroke across smoking status categories. Based on an alpha-level = 0.05, we would REJECT the null hypothesis and conclude there is a difference in frequency of stroke across smoking status. However, it should be of note that p-value = 0.04996, which is very close to 0.05, thus we should consider the possibility of type I error. 


Now let's look at hypertension

```{r}
stroke_data %>% 
  group_by(hypertension) %>%
  count(stroke) %>% 
  mutate(
    hypertension = factor(case_when(hypertension == 0 ~ "No Hypertension",
                                    hypertension == 1 ~ "Hypertension"))
  ) %>% 
  pivot_wider(
    names_from = hypertension,
    values_from=n
  ) %>% 
  adorn_totals("row") %>%
  knitr::kable()
```

From looking at the 2x2 table we see that there is a significant difference in odds of stroke based on presence of hypertension. We will further explore this via chi-squared test

```{r}
chisq.test(stroke_data$hypertension, stroke_data$stroke)
```
 Based on an alpha-level=0.05, we would REJECT the null hypothesis and conclude there is a difference in stroke prevalence across hypertension status. 
 
 Let's look at heart disease status now.
 
```{r}
stroke_data %>% 
  group_by(heart_disease) %>%
  count(stroke) %>% 
  mutate(
    stroke = factor(case_when(stroke == 0 ~ "No Stroke",
                              stroke == 1 ~ "Stroke")),
    heart_disease = factor(case_when(heart_disease == 0 ~ "No Heart Disease",
                                    heart_disease == 1 ~ "Heart Disease"))
  ) %>% 
  pivot_wider(
    names_from = stroke,
    values_from=n
  ) %>% 
  adorn_totals("row") %>%
  knitr::kable()
```
 From the table we see that there is difference of stroke prevalence across heart disease status. We will conduct chi-squared tests to further evaluate.
 
 
```{r}
chisq.test(stroke_data$heart_disease, stroke_data$stroke)
```
 Based on the alpha-level of 0.05, we would reject the null hypothesis and conclude there is a significant difference in stroke prevalence across stroke categories. 

```{r}
ggplot(stroke_data, aes(x=age, y=bmi)) +
  geom_point(aes(color=stroke)) +
  geom_smooth(se=FALSE)
```

```{r}
ggplot(stroke_data, aes(x=bmi, y=avg_glucose_level)) +
  geom_point(aes(color=stroke)) +
  geom_smooth(se=FALSE)
```



```{r}
x <- model.matrix(stroke ~ .,stroke_data)[,-1]
y <- stroke_data$stroke

featurePlot(x = x, y = y,
            scales = list(x = list(relation = "free"),
                          y = list(relation = "free")),
            plot= "density",
            labels = c("", "stroke"),
            pch = "|",
            auto.key = list(columns = 2))

```
So what did we learn from running the exploratory analysis. From looking at the various demographic social characteristics variables, we did not see much difference in stroke prevalence across categories such as type of work, gender, and type of residence. 

# Model Building

Now that we have done exploratory analysis, we will do model building.


Before we begin model building we will partition Data into Train and Test data. 

```{r}
stroke_df <- stroke_data %>% 
  mutate(
    stroke=factor(case_when(stroke == 0 ~ "no_stroke",
                            stroke == 1 ~ "stroke")),
    gender=factor(case_when(gender == "Female" ~ 0,
                            gender == "Male" ~ 1)),
    residence_type = factor(case_when(residence_type == "Rural" ~ 0,
                                      residence_type == "Urban" ~ 1)),
    smoking_status= factor(case_when(smoking_status == "never smoked" ~ 0,
                                     smoking_status == "formerly smoked" ~ 1,
                                     smoking_status == "smokes" ~ 2,
                                     smoking_status == "Unknown" ~ 3))) %>% 
  mutate(
    stroke = factor(stroke,levels = c("no_stroke", "stroke"))
  ) %>% 
  filter(gender != "Other") %>% 
  dplyr::select(-work_type)

part_index <- createDataPartition(y = stroke_df$stroke,
                                  p = 0.7,
                                  list = FALSE)

stroke_trn <- stroke_df[part_index, ]
stroke_tst <- stroke_df[-part_index, ]

stroke_x <- stroke_trn[1:9]
stroke_y <- stroke_trn$stroke

```

```{r}
    ever_married = factor(case_when(ever_married == 0 ~ "No",
                                    ever_married == 1 ~ "Yes")),
    hypertension = factor(case_when(hypertension == 0 ~ "No",
                                   hypertension == 1 ~ "Yes")),
    heart_disease = factor(case_when(heart_disease == 0 ~ "No",
                                     heart_disease == 1 ~ "Yes"))
    
```


```{r}
set.seed(10)
ctrl1 <- trainControl(method = "repeatedcv",
                      repeats = 5,
                      summaryFunction = twoClassSummary,
                      classProbs = TRUE)

stroke_glm <- train(x = stroke_x,
                    y = stroke_y,
                    method = "glm",
                    metric = "ROC",
                   trControl = ctrl1)

summary(stroke_glm)
```
From looking at this model we see the significant predicates( at alpha level = 0.05), are age, hypertension, and average glucose level. 

Now we will look at the performance of the model on the test data set. 
```{r}
tst_x <- stroke_tst[1:9]
tst_y <- stroke_tst$stroke


glm_pred <- predict(stroke_glm, newdata = tst_x)
conf_matrix <- confusionMatrix(data = as.factor(glm_pred),
                               reference = tst_y,
                               positive = "stroke")
                               
conf_matrix
```

From looking at the confusion matrix we see that the accuracy of the model is quite high at 95.79%. Thus the misclassification rate is 4.21%, which is quite low. Moreover the specificity is quite high at 99.92%, which means the likelihood of having a true negative from the model is 0.9992. However, the major issues with model stem from the extremely low sensitivity rate. The likelihood of the model having a true positive is 0.016. Now this low rate could be due to a multitude of issue. For one, a quick glance at the test data set shows tht the number of patients with stroke is significantly lower than the number of patients that had no stroke. We will conduct further testing by plotting the ROC curve

```{r}
glm_pred_prob <- predict(stroke_glm,
                    newdata = tst_x,
                    type = "prob")[,2]
roc_glm <- roc(tst_y, glm_pred_prob)

plot(roc_glm, col = "goldenrod3", legacy.axes = TRUE)


roc_glm$auc
```

The AUC of the model is equal to 0.8269. This value is actually quite good. To further understand which predictors could be significant, we will compare this model to other logistic regressions.


```{r}
set.seed(10)
stroke_gam <- train(x = stroke_x,
                  y = stroke_y,
                  method = "gam",
                  metric = "ROC",
                  trControl = ctrl1)

stroke_gam$finalModel

plot(stroke_gam$finalModel, select = 3)
```


```{r}
set.seed(10)
stroke_mars <- train(x = stroke_x,
                   y = stroke_y,
                   method = "earth",
                   tuneGrid = expand.grid(degree = 1:4,
                                          nprune = 2:15),
                   metric = "ROC",
                   trControl = ctrl1)

summary(stroke_mars)
plot(stroke_mars)
```


```{r}
stroke_mars$bestTune
coef(stroke_mars$finalModel)
```

```{r}
vip(stroke_mars$finalModel)
```

```{r}
set.seed(10)
stroke_lda <- lda(stroke ~ ., data = stroke_trn)
stroke_lda$scaling

plot(stroke_lda)
```



```{r}
set.seed(10)
stroke_lda <- train(x = stroke_x,
                    y = stroke_y,
                    method = "lda",
                    metric = "ROC",
                   trControl = ctrl1)
```



```{r}
res <- resamples(list(logistic = stroke_glm,
                      GAM = stroke_gam,
                      MARS = stroke_mars))
summary(res)

```

```{r}
bwplot(res, metric="ROC")
```

